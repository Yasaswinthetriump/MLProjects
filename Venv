Three-Month Project Plan: LLM GPU/CPU Cluster, OCP, and Run:AI Setup


---

Program Objective:

Set up a scalable and production-ready environment for training and inference of LLMs using H200 GPUs, CPU clusters for lighter workloads, integrated with OpenShift (OCP) and orchestrated via Run:AI.

Duration:

June 2025 â€“ August 2025

High-Level Themes:

1. Base Conda & Jupyter Environment for LLM Workloads


2. GPU and CPU Namespace Allocation & Isolation


3. Cluster-Wide Environment and Storage Management


4. Run:AI Orchestration and Training Pipelines


5. RAG/QnA Pipelines Integration (MVP)


6. Feature Flagged Testing and Rollout




---

Epics and Linked Stories

EPIC 1: Conda Base Environment & Dependency Management (JUNE)

Story 1.1: Finalize Conda YAML for GPU-Compatible Base Environment

Story 1.2: Resolve Artifactory-based Package Dependency Conflicts

Story 1.3: Validate CPU-Compatible Conda Environment (No CUDA)

Story 1.4: Conda Watch Analysis and Patch for Missing Libraries

Story 1.5: Document Channel Configuration Strategy (defaults / pip / internal)


EPIC 2: Namespace and Storage Setup for CPU & GPU Workloads (JUNE-JULY)

Story 2.1: Create GPU Namespace with PVC Allocation (24x H200)

Story 2.2: Create CPU Namespace with PEVC and Test LLMs on CPU

Story 2.3: Design Namespace Access Policy for JupyterHub Users

Story 2.4: Validate Volume Mounts and Usage Limits per Namespace

Story 2.5: Implement Resource Quotas and Alerts on Storage Limits


EPIC 3: JupyterHub and Cluster Level Integration (JULY)

Story 3.1: Enable JupyterHub Access to GPU/CPU Environments

Story 3.2: Validate Static vs Dynamic Kernel Allocation

Story 3.3: Implement Namespace-Based Environment Launch Buttons

Story 3.4: Document JupyterHub Profiles and Kernel Specs


EPIC 4: Run:AI Configuration & Model Execution (JULY)

Story 4.1: Integrate Run:AI with OCP Cluster for GPU Workloads

Story 4.2: Run PyTorch Transformer Model as Test Pipeline

Story 4.3: Test Job Scheduling and Node Utilization on Run:AI

Story 4.4: Create Docker Image for Trained LLM Model Deployment


EPIC 5: RAG/QnA MVP Pipeline & Chatbot Enablement (AUGUST)

Story 5.1: Build Basic RAG Pipeline using Chroma/pgvector

Story 5.2: Enable Retrieval + Embedding with BERT (CPU Compatible)

Story 5.3: Create QnA Chatbot Using Internal Model (LangChain / Transformers)

Story 5.4: Validate Inference Cost/Performance for GPU vs CPU

Story 5.5: Secure Access to Internal Confluence Pages via RAG


EPIC 6: Monitoring, Governance & Documentation (AUGUST)

Story 6.1: Set Up Grafana/Dashboards for GPU/CPU Utilization

Story 6.2: Configure Audit Logging for Jupyter and Run:AI Jobs

Story 6.3: Create a Central Knowledge Base in Confluence (Environment Setup + Known Issues + Best Practices)

Story 6.4: Conduct UAT and Sign-off for MVP1 Deliverables



---

Feature Links

Feature A: GPU Environment Lifecycle Management

Feature B: CPU Training Mode & Cost Optimization

Feature C: Auto-Mount PEVC for Model Logging

Feature D: Chatbot RAG Agent on Internal Docs (POC)

Feature E: Jupyter + Run:AI Shared Job Console Integration



---

Let me know if you want this broken into Kanban/JIRA format with story points or priority labels.

